# insert.py
import json
import os
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from config import DATA_FILES, VECTORSTORE_ROOT, EMBEDDING_API_URL, EMBEDDING_MODEL_NAME
from tqdm import tqdm

# å…¨å±€å…±äº«åµŒå…¥æ¨¡å‹
_embeddings = None


def get_embeddings():
    global _embeddings
    if _embeddings is None:
        _embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL_NAME,
            base_url=EMBEDDING_API_URL,
            api_key="none",
            check_embedding_ctx_length=False,
        )
    return _embeddings


def count_lines(file_path: str):
    """å¿«é€Ÿç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼ˆç”¨äº tqdm è¿›åº¦æ¡ï¼‰"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return sum(1 for _ in f)


def read_docs_from_file(file_path: str, source: str):
    """ç”Ÿæˆå™¨ï¼šé€è¡Œè¯»å–æ–‡ä»¶ï¼Œè¿”å› Document"""
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                if "code" not in item or "name" not in item:
                    continue

                metadata = {
                    "source": source,
                    **item
                }

                yield Document(
                    page_content=item["name"],
                    metadata=metadata
                )
            except json.JSONDecodeError:
                continue


def upsert_source(source: str, file_path: str, batch_size=1):
    """
    æ’å…¥æˆ–æ›´æ–° source
    âš ï¸ å…³é”®ä¿®æ”¹ï¼šé»˜è®¤ batch_size=1 ä»¥é¿å…æœåŠ¡ç«¯å¹¶å‘ä¹±åº
    """
    persist_dir = os.path.join(VECTORSTORE_ROOT, source)
    os.makedirs(VECTORSTORE_ROOT, exist_ok=True)

    try:
        total_lines = count_lines(file_path)
    except:
        total_lines = 30000

    # åˆå§‹åŒ–å‘é‡åº“
    vectorstore = Chroma(
        persist_directory=persist_dir,
        embedding_function=get_embeddings(),
        collection_metadata={"hnsw:space": "cosine"}
    )

    new_docs_buffer = []
    new_ids_buffer = []  # ğŸ‘ˆ æ–°å¢ï¼šç”¨äºå­˜å‚¨ ID
    inserted_count = 0

    print(f"ğŸš€ å¼€å§‹å¤„ç† {source}ï¼Œå¼ºåˆ¶æ¨¡å¼ batch_size={batch_size}...")

    with tqdm(
        total=total_lines,
        desc=f"ğŸ“ {source}",
        unit="æ¡",
        ncols=100
    ) as pbar:

        for doc in read_docs_from_file(file_path, source):
            # è·å–å”¯ä¸€ ID (ä½¿ç”¨ code)
            doc_id = doc.metadata["code"]

            new_docs_buffer.append(doc)
            new_ids_buffer.append(doc_id)

            # ç¼“å†²åŒºæ»¡ï¼ˆè¿™é‡Œé€šå¸¸æ˜¯ 1ï¼‰åˆ™å†™å…¥
            if len(new_docs_buffer) >= batch_size:
                # âœ… æ˜¾å¼ä¼ å…¥ idsï¼Œç¡®ä¿æ•°æ®ä¸€ä¸€å¯¹åº”ï¼Œä¸”æ”¯æŒæ›´æ–°è¦†ç›–
                vectorstore.add_documents(new_docs_buffer, ids=new_ids_buffer)
                
                inserted_count += len(new_docs_buffer)
                new_docs_buffer.clear()
                new_ids_buffer.clear()
                pbar.set_postfix({"inserted": inserted_count})
            
            pbar.update(1)

        # å¤„ç†å‰©ä½™æ•°æ®
        if new_docs_buffer:
            vectorstore.add_documents(new_docs_buffer, ids=new_ids_buffer)
            inserted_count += len(new_docs_buffer)

    # âŒ å·²åˆ é™¤ vectorstore.persist()ï¼ŒChroma æ–°ç‰ˆä¼šè‡ªåŠ¨ä¿å­˜
    tqdm.write(f"âœ… {source}: å®Œæˆï¼å…±å¤„ç† {inserted_count} æ¡ â†’ {persist_dir}")


def batch_upsert(batch_size=1):
    """æ‰¹é‡å¤„ç†æ‰€æœ‰ sourceï¼Œå¼ºåˆ¶ batch_size=1"""
    for source, file_path in DATA_FILES.items():
        try:
            upsert_source(source, file_path, batch_size=batch_size)
        except Exception as e:
            tqdm.write(f"âŒ {source} å¤±è´¥: {e}")


def test_query(source: str, query: str, k: int = 5, filter_dict: dict = None):
    """æŸ¥è¯¢æµ‹è¯•"""
    persist_dir = os.path.join(VECTORSTORE_ROOT, source)
    vectorstore = Chroma(
        persist_directory=persist_dir,
        embedding_function=get_embeddings(),
        collection_metadata={"hnsw:space": "cosine"}
    )

    print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
    if filter_dict:
        print(f"FilterWhere: {filter_dict}")
    
    try:
        docs_and_scores = vectorstore.similarity_search_with_score(
            query,
            k=k,
            filter=filter_dict
        )

        for i, (doc, score) in enumerate(docs_and_scores, 1):
            meta = doc.metadata
            print(f"\n{i}. ç›¸ä¼¼åº¦: {1 - score:.4f}") # Chromaè¿”å›çš„æ˜¯è·ç¦»ï¼Œ1-è·ç¦»=ç›¸ä¼¼åº¦
            print(f"   åç§°: {meta.get('name', 'N/A')}")
            print(f"   ç¼–ç : {meta.get('code', 'N/A')}")
            print(f"   ID: {doc.metadata.get('code')}") # éªŒè¯IDæ˜¯å¦æ­£ç¡®
    except Exception as e:
        print(f"æŸ¥è¯¢å¤±è´¥ (å¯èƒ½æ˜¯åº“ä¸ºç©º): {e}")


if __name__ == "__main__":
    # âš ï¸ å…³é”®ï¼šè¿™é‡Œå¿…é¡»è®¾ä¸º 1ï¼Œè§£å†³ vLLM ä¹±åºé—®é¢˜
    batch_upsert(batch_size=1)

    print("\n" + "="*60 + "\n")
    
    # ğŸ” éªŒè¯ä¿®å¤æ•ˆæœ
    test_query("ICD-10-fix", "å…ˆå¤©æ€§è€³å‰ç˜˜ç®¡", k=5)